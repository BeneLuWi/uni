\chapter{Durchführung}

\section{Verwendung der iRRAM-REALs}
\label{sec:irram}
Die Software-Bibliothek \verb+iRRAM+ \cite{Mller2009EnhancingIE} basiert auf Intervallen als Zahlentyp, um diese mit einer beliebigen Genauigkeit darstellen zu können. Zunächst wird mit Double-Präzision (64-Bit Zahlen) gerechnet, welche verwendet werden, bis das Ergebnis für die angefragte Präzision nicht mehr genau ausgegeben werden kann, beziehungsweise bis zu einer bestimmten Anzahl an Bits. Ist dies der Fall, geschieht eine Iteration mit einer erhöhten Genauigkeit, also längeren Zahlen, welche dann mit Hilfe von \verb+MPFR+ dargestellt werden.
Für eine solche Iteration werden gerade so viele Zwischenergebnisse während der Berechnung gespeichert, dass eine Wiederholung der Schritte mit höherer Präzision möglich ist. Da nicht die gesamte Berechnung in jeder Iteration wiederholt wird, müssen während der Laufzeit Sichtbarkeit von Variablen und Zwischenergebnisse für den Nutzer genau kontrolliert und unter Umständen beschränkt werden, da sonst unerwartetes Verhalten und Exceptions entstehen können, indem die zugegriffenen Werte gegebenfall nicht in der aktuellen Iteration existieren.


Einige der für rationale Zahlen zur Verfügung stehenden Funktionen, wie der Vergleich zweier Zahlen, sind mit reellen Zahlen nicht ohne Weiteres Möglich. Dies gilt insbesondere für den Test auf Gleichheit und die Vorzeichenfunktion $sign$. Bei all diesen Funktionen handelt es sich im Reellen (und bei der \verb+iRRAM+) um mehrwertige Funktionen, da sich das jeweilige Ergebnis mit veränderter Präzsion in der Darstellung der Zahlen ändern kann. Dieses Problem wird in \verb+hotm+ durch den \verb+SignType+ adressiert. Zusätzlich zu den Werten 'positiv' (=\verb+POS+) und 'negativ' (=\verb+NEG+),  kann die Vorzeichenfunktion \verb+sign+ den Wert 'ambivalent' (=\verb+AMBI+) ausgeben, wenn nicht entscheidbar ist, wo genau die reelle Zahl um die Null liegt. Hierfür erhält die Vorzeichenfunktion einen Parameter, der den Bereich er Unsicherheit definiert. Ist der Ausgabewert 'ambivalent', so wird in den Funktionen, welche die Vorzeichenfunktion aufrufen, der schlechteste Fall im Hinblick auf die Genauigkeit des Ergebnisses angenommen. 


Eine weitere Besonderheit ergibt sich aus dem Aufbau der Zahlen, denn es entstehen zwei Intervall-`Ebenen`: Zum Einen, die Darstellung des Koeffizienten als Intervall aus Mitte und Radius. Zum Anderen die Darstellung von Mitte und Radius, als \verb+iRRAM-REAL+, also auch wiederum jeweils als Intervall mit Wert und Fehler, wie in Grafik \ref{fig:levels} zu sehen ist. Im Vergleich zu den \verb+mpq-RATIONALS+ erhöht sich hier zwar die Komplexität deutlich, allerdings lassen sich Ungenauigkeiten sehr genau steuern, indem zum Beispiel der Rechenfehler des Mittelpunkts eines Koeffizienten auf den Radius 'verlagert' wird. So vergrößert sich zwar der Radius des Koeffizienten, welcher dadurch ungenauer wird, jedoch verkleinert sich der Rechenfehler auf der Zahlenebene der \verb+REAL+s. Ein ählicher Effekt sollte sich durch Rundung auch mit den \verb+mpq-RATIONALS+ erreichen lassen. 


Das Verlagern der Ungenauigkeit ($e$ in der Grafik \ref{fig:levels}) auf den Wert des Radius' ($v$ in der Grafik), \textit{Micro-Housekeeping} genannt, verringert die Intervallbreite und erzeugt Punktintervalle auf der Zahlenebene, allerdings werden die Intervalle auf der Intervallebene breiter. Hier greift dann wiederum der Polish-Mechanismus, der auf Polynomebene Monome hinzufügt, um aus den zu groß gewordenen Intervallen wiederum Punktintervalle zu machen, \textit{Macro-Housekeeping} genannt. So wird der Rechenfehler von der untersten Ebene bis zur Polynomebene propagiert.
Diese Housekeeping-Funktionen werden durch Parameter gesteuert, die bestimmen, ab wann ein Intervall zu breit ist und die jeweilige Prozedur angewandt werden soll.

\begin{figure}[ht]
\label{fig:levels}
\begin{center}
 
 \input{graphs/levels.tex}
 \caption{Ebenen der Polynomdarstellung mit REALs (informell)}
 \end{center}
\end{figure}


\section{Genauigkeitsmodell}
\label{sec:precision}
Die \verb+iRRAM+ bietet die Möglichkeit, zwischen \textit{absoluter} und \textit{relativer} Genauigkeit zu unterscheiden. Absolute Genauigkeit bedeutet, dass beim Verrechnen zweier \verb+REAL+s ein global verwendeter Wert für die Genauigkeit zugrunde liegt und die Zahlen dementsprechen skaliert werden. Bei relativer Genauigkeit hängt die verwendete Genauigkeit von der tatsächlichen Größe der jeweiligen Zahlen ab. Diese kann zum Beispiel durch punktuelle Anwendung von Micro-Housekeeping stark variieren, weshalb die Verwendung von relativer Genauigkeit in der Praxis besser funktioniert.


Für der Vergleich wurde $x_{1000}$ mit $x_n = c \cdot x_{n-1} \cdot (1 - x_{n-1})$ mit der folgenden Konfiguration berechnet:
\begin{enumerate}
 \item Nur quadratisches Sweeping zur 0,
 \item Entfernen aller Fehlersymbole, bis auf die 3 zuletzt hinzugefügten,
 \item Micro-Housekeeping (Cleaning) ab einem Fehler $\chi$ auf Zahlenebene $>10^{-100}$,
 \item Macro-Housekeeping (Splitting) ab einer Intervallbreite $\psi$ $>10^{-50}$,
 \item $x_0 = 0,5$ und
 \item Ausgabe des Ergebnisses auf 20 Stellen genau.
\end{enumerate}
Zu Beginn einer jeden Iteration werden zunächst die höhergradigen Monome gesweept (1.), dann die älteren Fehlersymbole (2.). Im nächsten Schritt wird der Fehler und die Intervalle bereinigt (3.) und zuletzt das Polynom poliert (4.), bevor erneut multipliziert wird. 

Für $c = 3.25$ konvergiert die Funktion gegen zwei Werte und lässt sich sehr genau bestimmen.

\begin{center}
\begin{tabular}{|r|c|c|}
\hline
\multicolumn{3}{|c|}{$x_{1000}$ mit $c=3.25$}\\
\hline
 mit Housekeeping&relative Genauigkeit & absolute Genauigkeit \\
 \hline
 \hline
 Anzahl Splits & 142 & 6961 \\
 Präzision (Bits) & \verb+double+ & 136 \\
 CPU-Zeit & 0.8s & 4.43s\\
 \hline
\end{tabular}
\end{center}

Wie in der Tabelle zu sehen ist, benötigt die Berechnung mit relativer Genauigkeit erheblich weniger Zeit und Bits um ein Ergebnis mit der geforderten Auflösung zu erhalten. Wird auf die Housekeeping-Methoden verzichtet, zeigt sich dieser massive Unterschied jedoch nicht:

\begin{center}
\begin{tabular}{|r|c|c|}
\hline
\multicolumn{3}{|c|}{$x_{1000}$ mit $c=3.25$}\\
\hline
 ohne Housekeeping &relative Genauigkeit & absolute Genauigkeit \\
 \hline
 \hline
 Anzahl Splits & -& -\\
 Präzision (Bits) & 7440 & 7440 \\
 CPU-Zeit & 0.120s & 0.118s\\
 \hline
\end{tabular}
\end{center}

Die Ergenisse werden ohne Housekeeping erheblich schneller berechnet, allerdings auf Kosten der benötigten Bits für die Zahlendarstellung. Da hier kein Polieren geschieht, wird auch kein Taylormodell als solches initialisiert, das heißt, es werden keine Fehlersymbole eingesetzt und rein auf der Ebene der reellen Zahlen der \verb+iRRAM+ gerechnet.

Für $c=3.75$, einem Wert, bei dem die Funktion chaotisches Verhalten aufweist und zwischen vielen Werten hin- und herspringt, ist erkennbar, dass ein Rechnen mit diesen Housekeeping-Parametern und absoluter Genauigkeit nicht praktikabel ist, da die Berechnung bereits nach 60 Iterationen eine Laufzeit von über 15 Sekunden hat, welche scheinbar exponentiell steigt. Für relative Genauigkeit ist die Berechnung zwar zeitaufwändig, liefert jedoch ein (korrektes) Ergebnis und benötigt weniger Bits, als eine Berechnung ohne Housekeeping. Die Parameter mussten hier auf $\psi = 10^{-140}$ und $\chi = 10^ {-150}$ angepasst werden, damit die Grenzwerte erreicht werden.

\begin{center}
\begin{tabular}{|r|c|c|}
\hline
\multicolumn{3}{|c|}{$x_{1000}$ mit $c=3.75$}\\
\hline
 mit Housekeeping &relative Genauigkeit & absolute Genauigkeit \\
 \hline
 \hline
 Anzahl Splits & 64 & ?\\
 Präzision (Bits) & 5894 & ?\\
 CPU-Zeit & 8.2s & >60s\\
 \hline
\end{tabular}
\end{center}

Wie oben zeigt sich zwischen relativer und absoluter Genauigkeit bei der Verwendung ohne Housekeeping-Methoden kein nennenswerter Unterschied. Zudem scheint bei der hier verwendeten Konfiguration lediglich ein Berechnungsoverhead zu entstehen, welcher keine Auswirkungen auf die Qualität des Ergebnisses hat.


\begin{center}
\begin{tabular}{|r|c|c|}
\hline
\multicolumn{3}{|c|}{$x_{1000}$ mit $c=3.75$}\\
\hline
 ohne Housekeeping &relative Genauigkeit & absolute Genauigkeit \\
 \hline
 \hline
 Anzahl Splits & - & -\\
 Präzision (Bits) & 7440 & 7440\\
 CPU-Zeit & 0.12s & 0.12s\\
 \hline
\end{tabular}
\end{center}


Insgesamt lässt sich beobachten, dass die Berechnung der Interationen mit relativer Genauigkeit in den untersuchten Fällen bessere Ergebnisse liefert, weshalb diese im Folgenden zugrunde liegt.

\section{Housekeeping}
\label{sec:housekeeping}
Die Werte der Parameter für die Housekeeping-Methoden hängen unmittelbar voneinander ab: Je größer der Wert des Cleaning $\chi$, also as Umlagern des Rechenfehlers eines Koeffizienten auf den Mittelpunkt des Radius', desto schneller wird der Grenzwert des Splitings $\psi$, also des Einführens eines neuen Fehlersymbols erreicht.

Für $c=3.25$ kann $x_{1000}$ mit $\psi = 10^{-50}$ in \verb+double+-Präzision berechnet werden. Je mehr sich $\psi$ und $\chi$ annähern, desto öfter überschreitet der Radius eines Koeffizienten den Grenzwert $\psi$ und wird poliert, wie in Grafik \ref{fig:polish} zu sehen ist.

\begin{figure}[ht]
    \centering
    \begin{tikzpicture}
    \begin{axis}[
    width=0.7\textwidth,
    height=0.5\textwidth,
        xlabel={Grenzwert $\chi$ [$\cdot 10^{x}$]},
        ylabel={Anzahl der Splits},
        legend pos=north west,
        xmin=-200,xmax=-50,
        ymajorgrids=true,
        grid style=dashed
    ]
    \addplot[
        color=blue,
        ]
        coordinates {
(-200,103)
(-190,103)
(-180,103)
(-170,103)
(-160,103)
(-150,103)
(-140,111)
(-130,120)
(-120,110)
(-110,112)
(-100,142)
(-90,191)
(-80,223)
(-70,369)
(-60,650)
(-50,1110)
        };  

        
    \end{axis}
    \end{tikzpicture}
    \caption[Grenzwertvergleich]{Anzahl der Splits für $x_{1000}$ bei \\ $\psi = 10^{-50}$ und wachsendem $\chi$}
    \label{fig:polish}
\end{figure}


Ab einem $\chi < 10^{-70}$ und $\psi < 0.1$ und mit $\chi < 10^{-50}$ mit $\psi < 10^{-50}$ mit \verb+double+-Präzision genau berechenbar. In Tabelle \ref{tab:thresholds} sind dies die Werte mit 50 Bit Genauigkeit.

Für $c = 3.75$ ist $\chi=10^{-150}$ und $\psi=10^{-140}$ eine gute Konfiguration mit 5894 benötigten Bits für $x_{1000}$. Dieser Wert wird bei keiner der getesteten Konfigurationen unterschritten, was mit Hilfe des Lyapunov Exponenten für diese spezielle Iteration erklärt werden kann. Der Lyapunov Exponent gibt eine Annäherung an die untere Schranke der pro Iteration durch Rundungsfehler verlorenen Bits an, die für $c=3.25$ niedriger ist, als für $c=3.75$, welhalb eine Berechnung mit \verb+double+-Präzision möglich ist \cite{DBLP:spandl}.

\begin{center}
\begin{table}
\renewcommand{\arraystretch}{1.2}
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|}
\hline
  \begin{tabular}[t]{@{}lr@{}}
      $\cdot 10^{-x}$     & $\psi$  \\
     $\chi$&
  \end{tabular}& 0 & 10 & 20 & 30 & 40 & 50 & 60 & 70 & 80 & 90& 100 \\
\hline 
0&
\shortstack{ \\7440\\ 0}& 
\shortstack{ \\7440\\ 0}& 
\shortstack{ \\7440\\ 0}& 
\shortstack{ \\7440\\ 0}& 
\shortstack{ \\7440\\ 0}& 
\shortstack{ \\7440\\ 0}& 
\shortstack{ \\7440\\ 0}& 
\shortstack{ \\7440\\ 0}& 
\shortstack{ \\7440\\ 0}& 
\shortstack{ \\7440\\ 0}& 
\shortstack{ \\7440\\ 0} \\
\hline
10&
\shortstack{ \\7440\\ 0}& 
\shortstack{ \\9372\\ 0}& 
\shortstack{ \\2876\\ 172}& 
\shortstack{ \\2876\\ 124}& 
\shortstack{ \\2876\\ 113}& 
\shortstack{ \\2876\\ 103}& 
\shortstack{ \\2876\\ 101}& 
\shortstack{ \\2876\\ 107}& 
\shortstack{ \\2876\\ 124}& 
\shortstack{ \\2876\\ 130}& 
\shortstack{ \\2876\\ 142}\\ 
\hline
20&
\shortstack{ \\7440\\ 0}& 
\shortstack{ \\242\\ 1277}& 
\shortstack{ \\50\\ 2780}& 
\shortstack{ \\748\\ 565}& 
\shortstack{ \\748\\ 555}& 
\shortstack{ \\375\\ 1594}& 
\shortstack{ \\375\\ 1880}& 
\shortstack{ \\748\\ 913}& 
\shortstack{ \\1008\\ 672}& 
\shortstack{ \\375\\ 2790}& 
\shortstack{ \\1008\\ 793}\\
\hline
30&
\shortstack{ \\7440\\ 0}& 
\shortstack{ \\242\\ 1231}& 
\shortstack{ \\2876\\ 178}& 
\shortstack{ \\2876\\ 103}& 
\shortstack{ \\2876\\ 78}& 
\shortstack{ \\2876\\ 74}& 
\shortstack{ \\2876\\ 55}& 
\shortstack{ \\2876\\ 51}& 
\shortstack{ \\2876\\ 49}& 
\shortstack{ \\2876\\ 51}& 
\shortstack{ \\2876\\ 55}\\
\hline
40&
\shortstack{ \\7440\\ 0}& 
\shortstack{ \\50\\ 1256}& 
\shortstack{ \\2876\\ 176}& 
\shortstack{ \\2876\\ 103}& 
\shortstack{ \\748\\ 830}& 
\shortstack{ \\2876\\ 59}& 
\shortstack{ \\2876\\ 51}& 
\shortstack{ \\2876\\ 48}& 
\shortstack{ \\2876\\ 45}& 
\shortstack{ \\2876\\ 47}& 
\shortstack{ \\1008\\ 3249}\\
\hline
50&
\shortstack{ \\7440\\ 0}& 
\shortstack{ \\541\\ 1092}& 
\shortstack{ \\2876\\ 157}& 
\shortstack{ \\2876\\ 100}& 
\shortstack{ \\136\\ 626}& 
\shortstack{ \\50\\ 1110}& 
\shortstack{ \\50\\ 2058}& 
\shortstack{ \\50\\ 3061}& 
\shortstack{ \\50\\ 3104}& 
\shortstack{ \\50\\ 3252}& 
\shortstack{ \\50\\ 4307}\\
\hline
60&
\shortstack{ \\7440\\ 0}& 
\shortstack{ \\50\\ 1330}& 
\shortstack{ \\2876\\ 197}& 
\shortstack{ \\2876\\ 102}& 
\shortstack{ \\50\\ 360}& 
\shortstack{ \\50\\ 650}& 
\shortstack{ \\50\\ 793}& 
\shortstack{ \\50\\ 1478}& 
\shortstack{ \\50\\ 2831}& 
\shortstack{ \\50\\ 3006}& 
\shortstack{ \\50\\ 3046}\\
\hline
70&
\shortstack{ \\7440\\ 0}& 
\shortstack{ \\50\\ 1209}& 
\shortstack{ \\50\\ 305}& 
\shortstack{ \\50\\ 182}& 
\shortstack{ \\50\\ 244}& 
\shortstack{ \\50\\ 369}& 
\shortstack{ \\50\\ 585}& 
\shortstack{ \\50\\ 730}& 
\shortstack{ \\50\\ 1498}& 
\shortstack{ \\50\\ 2198}& 
\shortstack{ \\50\\ 3052}\\
\hline
80&
\shortstack{ \\7440\\ 0}& 
\shortstack{ \\50\\ 1265}& 
\shortstack{ \\50\\ 283}& 
\shortstack{ \\50\\ 209}& 
\shortstack{ \\50\\ 193}& 
\shortstack{ \\50\\ 223}& 
\shortstack{ \\50\\ 346}& 
\shortstack{ \\50\\ 618}& 
\shortstack{ \\50\\ 619}& 
\shortstack{ \\50\\ 2099}& 
\shortstack{ \\50\\ 2540}\\ 
\hline
90&
\shortstack{ \\7440\\ 0}& 
\shortstack{ \\50\\ 1279}& 
\shortstack{ \\50\\ 285}& 
\shortstack{ \\50\\ 179}& 
\shortstack{ \\50\\ 133}& 
\shortstack{ \\50\\ 191}& 
\shortstack{ \\50\\ 229}& 
\shortstack{ \\50\\ 365}& 
\shortstack{ \\50\\ 651}& 
\shortstack{ \\50\\ 851}& 
\shortstack{ \\50\\ 1760}\\
\hline
100&
\shortstack{ \\7440\\ 0}& 
\shortstack{ \\50\\ 1181}& 
\shortstack{ \\50\\ 332}& 
\shortstack{ \\50\\ 184}& 
\shortstack{ \\50\\ 134}& 
\shortstack{ \\50\\ 142}& 
\shortstack{ \\50\\ 199}& 
\shortstack{ \\50\\ 236}& 
\shortstack{ \\50\\ 391}& 
\shortstack{ \\50\\ 571}& 
\shortstack{ \\50\\ 709}\\
\hline
\end{tabular}

\caption[Kombinationsmöglichkeiten der Grenzwerte für Housekeeping-Methoden]{Kombinationsmöglichkeiten der Grenzwerte für Housekeeping-Methoden. Die Zelle besteht aus den benötigten Bits für die Ausgabe und der Anzahl der Splits für die Berechnung von $x_{1000}$ für $c=3.25$, wie in Kapitel \ref{sec:precision} beschrieben}
\label{tab:thresholds}
\end{table}
\end{center}



\section{Messungenauigkeiten}

Die feingranularen Konfigurationsmöglichkeiten ermögleichen eine manuelle Eingabe eines Rechenfehlers zu Beginn der Berechnung, wodurch bespielsweise das Rechnen mit ungenauen Werten simuliert werden kann. Für $x_n = x_{n-1} \cdot c \cdot (1 - x_{n-1})$ und $x_0 = 0.5$ kann $x_0$ als Taylormodell mit Intervallkoeffizienteneinen solchen Rechenfehler an verschiedenen Stellen enthalten:
\begin{align}  
    x_0 &= [0 \pm 0] + [1 \pm 0] \cdot \lambda & \lambda \in [0.5 \pm \varepsilon] \label{tm1} \\  
    x_0 &= [0.5 \pm 0] + [1 \pm 0] \cdot \lambda & \lambda \in [0 \pm \varepsilon] \label{tm2} \\  
    x_0 &= [0.5 \pm 0] + [\varepsilon \pm 0] \cdot \lambda & \lambda \in [0 \pm 1] \label{tm3} \\   
    x_0 &= [0 \pm 0] + [0.5 \pm \varepsilon] \cdot \lambda & \lambda \in [1 \pm 0] \label{tm4} \\    
    x_0 &= [0.5 \pm 0] + [0 \pm \varepsilon] \cdot \lambda & \lambda \in [1 \pm 0] \label{tm5} \\ 
    x_0 &= [0.5 \pm \varepsilon] \label{tm6}
\end{align}

Bei einem Fehler $\varepsilon = 10^{-200}$ und $c=3.25$ kann bei \ref{tm1} und \ref{tm4} bereits nach wenigen Iterationen keine Aussage mehr über das Ergebnis getroffen werden, da die Intervalle zu stark wachsen, beziehungsweise zu breit werden. Mit den restlichen Definitionen kann die Berechnung auch bei höheren Iterationszahlen ($n=1000, n=10000$) mit \verb+double+-Präzision ausgeführt werden.  
   

Für die Iteration mit $c=3.75$, also einem Wert mit größerem Informationsverlust pro Iteration verhalten sich die Berechnungen etwas anders. Mit derselben manuell auf 743 Bit fixierten Genauigkeit entwickelt sich die Intervallbreite des Ergebnisses stark abhängig von der Darstellung von $x_0$, wie in Grafik \ref{fig:tm} zu sehen ist.  
          
\input{src/375.tex}

Die Intervallbreite wächst ab einer bestimmten Iterationszahl exponentiell an und lässt keine genaue Ausgabe des Ergebnisses für die aktuelle Präzision mehr zu. Für das Taylormodell \ref{tm2} wächst das Intervall hierbei am spätesten, die Ergebnisse bleiben also länger aussagekräftig. Wie auch bei $c=3.25$ sind die Taylormodelle \ref{tm1} und \ref{tm4}, also solche mit 0-Kernel am schlechtesten geeignet.


Als Genauigkeit wurde 743 Bit gewählt, da sich hier große Unterschiede zwischen den Taylormodell-Definitionen gezeigt haben. Bei einem kleineren Wert zeigten sich diese zwar auch, jedoch war dann keine so hohe Iterationszahl möglich.




\section{Lineare Taylormodelle}
Mit der in Abschnitt \ref{sec:precision} beschriebenen Konfiguration ist die Berechnung von $x_{1000}$ mit $c=3.25$ und linearen Taylormodellen in der \verb+hotm+-Implementierung nicht mit \verb+double+-Präzision möglich, da die Intervalle bereits nach zirka 70 Iterationen zu groß werden, um ein Ergebnis zu bestimmen.

Ein Grund hierfür könnte sein, dass die Cleaning Operation nur für den Kernel verwendet werden kann, da sonst ein nichtlineares Monom entstünde. So kann der Fehler auf Zahlenebene (siehe Grafik \ref{fig:levels}) ungehindert wachsen.









% \begin{center}
% \begin{tabular}{|c|c|c|c|}
% \hline
% \multicolumn{4}{|c|}{$x_{1000}$ mit $c=3.75$}\\
% \hline 
%  $\psi$ & $\chi$ & Bits & Polishs\\
%  \hline
%     & $10^{-50}$ & \verb+double+ & 1110 \\  
%     & $10^{-40}$ & 2876 & 2907\\
% $10^{-50}$& $10^{-30}$ & 2876 & 3890\\
%     & $10^{-25}$ & 2876 & 5542\\ 
%     & $10^{-24}$ & 2876 & 5998\\ 
%     & $10^{-20}$ & 375 & 1594\\ 
%     & $10^{-15}$ & 2876 & 72\\
%     & $10^{-10}$ & 2876 &103\\    
%     & $10^{-5}$ & 7440 & 0\\
%  \hline
% \end{tabular}
% \end{center}

% 
% \begin{table}[ht]
%     \centering
%     \def\arraystretch{1.3}
%     \begin{tabular}{r|c|c|c}
%     Iterationen &Metrik & absolut & relativ \\
%     \hline
%     $n=60$ & CPU-Zeit & $0.08s$ &  $3.21s$       \\
%            & Genauigkeit & \verb+double+ &   $2^{-748}$      \\
%            & Anzahl Polishs & 45 &    374     \\ 
%     \hline
%     $n=65$ & CPU-Zeit & $0.14s$ &  $18.06s$       \\
%            & Genauigkeit & \verb+double+ &  $2^{-748}$       \\
%            & Anzahl Polishs & 71 &   409      \\
%     \hline
%     $n=66$ & CPU-Zeit & $0.18s$ & $69s$\\
%            & Genauigkeit & \verb+double+ &   $2^{-748}$      \\
%            & Anzahl Polishs & 77 &    416     \\
%     \hline
%     $n=67$ & CPU-Zeit & $0.23s$ & $180s$ \\
%            & Genauigkeit & \verb+double+ &   $2^{-748}$      \\
%            & Anzahl Polishs & 84 &     423    \\
%     \hline
%     $n=100$ & CPU-Zeit & $0.38s$ & $>180s$ \\
%            & Genauigkeit & $2^{-748}$ &??\\
%            & Anzahl Polishs & 314 &??\\
%     \hline
%     $n=500$ & CPU-Zeit & $2.83s$ &$>180s$\\
%            & Genauigkeit & $2^{-2876}$ &??\\
%            & Anzahl Polishs & 3939 &??\\
%     \end{tabular}
%     \caption[Vergleich von Genauigkeitsmodellen]{Vergleich vom Rechnen mit absoluter und relativer Genauigkeit}
%     \label{tab:precision}
% \end{table}
