%% zusammenf.tex
%% $Id: zusammenf.tex 61 2012-05-03 13:58:03Z bless $
%%

\chapter{Fazit}
\label{ch:fazit}
%% ==============================

\section{Diskussion}
Der Schritt von linearen zu nichtlinearen Taylormodellen erhöht in der Praxis zwar deren Potential, Rundungsfehler zu kontrollieren, jedoch erhöht sich die Komplexität der Berechnung und der Strukturen um ein vielfaches, was sich in der Laufzeit, als auch im Aufwand für die Programmierung niederschlägt. Dadurch, dass jede Zahl der \verb+HOTM+ von einem Intervall dargestellt wird, beziehungsweise eine \verb+iRRAM-REAL+ ist, sorgt jede arithmetische Operation wegen der Eigenschaften der Intervallarithmetik zu einer Überschätzung. Wachsen nun die Polynome in ihrer Länge, kann zwar das Abhängigkeitsproblem der Variablen angegangen werden, jedoch erhöht sich auch die Anzahl der ausgeführten Operationen, was letztendlich die Ausdehnung der Taylormodelle dominiert, wie in Kapitel \ref{ch:Evaluierung} zu sehen ist. 

Durch den vielschichtigen Aufbau des Zahlentyps \verb+HOTM+, ist es meist schwierig zu sagen, ob eine Konfiguration optimal ist, da sehr viele Faktoren eine Rolle spielen. Um beispielsweise die Ausdehnung eines Rechtecks zu untersuchen, mussten starke Einschränkungen an den unterliegenden \verb+iRRAM+-Iterationen vorgenommen werden. Darunter fällt die Fixierung der Genauigkeit der \verb+REAL+s, die eigentlich dynamisch zu Laufzeit angepasst wird. Ohne eine solche Fixierung und andere Beschränkungen, war es nicht möglich, die experimentellen Ergebnisse klar zu deuten und die Einflüsse voneinander zu trennen. Diese Einschränkungen entstanden aus einer längeren Auseinandersetzung mit dem zu Grunde liegenden Problem und dem Wissen darüber, wann ein Rechteck bestimmter Größe, bei einer gewissen Genauigkeit die Fangzone in der H\e non-Abbildung verlässt. Um die \verb+HOTM+ als generischen Zahlentypen für Intervallarithmetik verwenden zu können, sind der Zeit noch solche Informationen und entsprechende Anpassungen nötig, da Berechnungen in einigen Fällen sonst nicht möglich sind.


Die Durchführung der Experimente und die Bewertung der Performanz der Parameter für die \verb+HOTM+ hat sich in dieser Arbeit auf Untersuchungen der H\e non-Abbildung beschränkt. Wenngleich die gefundenen Konfigurationen gute Ergebnisse lieferten, ist nicht garantiert, dass diese auch in anderen Fällen, in anderen Anwendungen die besten Ergebnisse liefern. Daher müsste besonders die Einstellung über die Anzahl der erhaltenen Fehlersymbole getestet und eventuell angepasst werden. Für diesen Zweck wurde dem Konstruktor der Taylormodelle und der Initialisierungsfunktion eine entsprechende Konfigurationsmöglichkeit hinzugefügt. 

\section{Ausblick}
In dieser Arbeit wurde eine grundlegende Implementierung für das Rechnen mit nichtlinearen Taylormodellen auf den reellen Zahlen beschrieben und angefertigt. An verschiedenen Stellen ist es jedoch möglich und teils notwendig, die Arbeit fortzuführen, was jedoch im Rahmen der Anfertigung der Masterarbeit nicht mehr möglich war.

\subsection*{Partitionierung}
In Kapitel \ref{sec:darstellung} wurde eube Art der Darstellung der Taylormodelle vorgestellt. Für jedes Fehlersymbol wird dessen Supportintervall in $n$ gleich große Partitionen unterteilt und eingesetzt, sodass sich für ein $d$-dimensionales Taylormodell $n^d$ ausgewertete Intervalle ergeben. Dieser sehr naive Algorithmus dient in dieser Arbeit lediglich der grafischen Darstellung der Taylormodelle, könnte jedoch als basis für einen Partitionsalgorithmus für eine parallele Berechnung größerer Taylormodelle dienen. Wie oben beschriebem, sorgt eine gewisse Größe der Taylormodelle bei der H\e non-Abbildung dazu, dass diese in wenigen Iterationen gegen $\infty$ laufen. Verhindert man dies jedoch durch eine vorherige Unterteilung, so könnte die Berechnung fortgeführt werden, indem die Informationen über das Wachstum der Intervalle aus Kapitel \ref{sec:anwendung} für die Partitionsgröße verwendet wird.


\subsection*{Laufzeitoptimierung} 
In der Implementierung der \verb+HOTM+ geschehen derzeit noch viele sich wiederholende Berechnungen, wie das Sweepen eines Fehlersymbols in mehreren Monomen und deren Sortierung nach der Größe des Supportintervalls. Für die Ergebnisse dieser Berechnungen würde sich ein Caching-Mechanismus, der eine begrenzte Menge an Informationen erhält anbieten, um Redundanz der teuren Rechenschritte zu verhindern. Da nicht jedes Zwischenergebnis gespeichert werden kann, müsste beispielsweise eine Priorisierung besonders häufig auftretender Fehlersymbole geschehen.


\subsection*{Sweeping-Heuristiken}
Statt die Fehlersymbole eines Monoms der Größe nach aufsteigend zu sortieren und dann zu sweepen, könnten andere Heuristiken bessere Ergbenisse liefern. Eine denkbare Lösung würde ein Backtracingalgorithmus darstellen, der bei einem Blow-up der Intervallgrenzen bestimmte Fehlersymbole vom Sweeping ausschließt und die vorhergegangen Schritte rückgängig macht.


\subsection*{Schnittstellen}
Die Implementierung der \verb+iRRAM+, beziehungsweise der \verb+HOTM+ bietet ein sehr hohes Abstrantionsniveau für die komplexen Berechnungen, die damit ausgeführt werden, sodass die reellen Zahlen und die nichtlinearen Taylormodelle als reguläre Zahlentypen verwendbar sind. Neben der reinen Entwicklungsarbeit, stellt allerdings auch die Verbreitung der Ideen und der Programme als solches ein Problem dar, da sich deren Verwendung auf \verb.C++.-Programme beschränkt. Eine Schnittstelle zu anderen Programmiersprachen, wie beispielsweise in der \verb+Python+-Bibliothek \verb+numpy+ \cite{harris2020array}, eigentlich in \verb+FORTRAN+ implementiert, würde die Verwendung in der populärsten Programmiersprache\footnote{\url{https://de.statista.com/infografik/22669/popularitaet-von-programmiersprachen/} (Stand 26.03.2021)} eröffnen.  


Für die Umsetzung wäre eine mögliche Lösung, die \verb+iRRAM+, beziehungsweise \verb+HOTM+ in einem eigenen Thread laufen zu lassen und mit asynchronen Anfragen Ergebnisse für Berechnungen zu erhalten.

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "thesis"
%%% End: 
